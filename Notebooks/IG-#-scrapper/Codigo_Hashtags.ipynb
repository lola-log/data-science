{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWITTER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUENTE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def tweets_n_edges(tweet_file):\n",
    "    tweets=[]\n",
    "    edges=[]\n",
    "\n",
    "    for i in open(tweet_file,\"r\"):\n",
    "        if i==\"\\n\":\n",
    "            next\n",
    "        else:\n",
    "            try:\n",
    "                tweet = json.JSONDecoder().raw_decode(i)[0]\n",
    "                usr_mentions= tweet['entities']['user_mentions']\n",
    "                if len(usr_mentions)>0:\n",
    "                    for ii in usr_mentions:\n",
    "                        if tweet['user']['screen_name'] != ii['screen_name']:\n",
    "                            edges.append((tweet['user']['screen_name'], ii['screen_name']))\n",
    "                tweets.append(tweet)\n",
    "            except: # if no user mentions, or something unexpected\n",
    "                continue\n",
    "\n",
    "    return (tweets,edges)\n",
    "\n",
    "tweets,edges = tweets_n_edges(\"data_science_twitter.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"There are %s tweets about data science this week, and %s user mentions!\" % ( len(tweets), len(edges) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G=nx.DiGraph() # initiate a directed graph\n",
    "G.add_edges_from(edges) # add edges to the graph from user mentions\n",
    "ev_cent=nx.eigenvector_centrality(G,max_iter=10000) # compute eigenvector centrality\n",
    "\n",
    "ev_tuple = []\n",
    "for i in ev_cent.keys():\n",
    "    ev_tuple.append((i,ev_cent[i]))\n",
    "    \n",
    "zip(range(1,11)[::-1],sorted(ev_tuple,key=lambda x: x[1])[-10:])[::-1] # get the top 10 network influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "def get_communities(tweets, edges):\n",
    "    G_un=nx.Graph()\n",
    "    G_un.add_edges_from(edges)\n",
    "    parts = community.best_partition(G_un)\n",
    "    values = [parts.get(node) for node in G_un.nodes()]\n",
    "\n",
    "    communities = {}\n",
    "\n",
    "    for i in tweets:\n",
    "        screen_name = i['user']['screen_name'].encode(\"ascii\",\"ignore\")\n",
    "        raw_text = i['text'].encode(\"ascii\",\"ignore\")\n",
    "        if screen_name in parts.keys() and i['lang'] in ('en','und'): # get english tweets\n",
    "            comm_num = parts[screen_name]\n",
    "            if comm_num in communities.keys():\n",
    "                if screen_name in communities[comm_num].keys():\n",
    "                    text = communities[comm_num][screen_name]['raw_text']\n",
    "                    communities[comm_num][screen_name]['n_tweets'] += 1\n",
    "                    communities[comm_num][screen_name]['raw_text'] = ' '.join([text, raw_text]) \n",
    "                else:\n",
    "                    communities[comm_num][screen_name] = {\n",
    "                        'raw_text' : raw_text,\n",
    "                        'n_tweets' : 1 \n",
    "                    }\n",
    "            else:\n",
    "                communities[comm_num] = {}\n",
    "                communities[comm_num][screen_name] = {\n",
    "                    'raw_text' : raw_text,\n",
    "                    'n_tweets' : 1 \n",
    "                }\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return communities\n",
    "\n",
    "communities = get_communities(tweets,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_size = []\n",
    "for i in communities.keys():\n",
    "    community_size.append((i,len(communities[i].keys())))\n",
    "\n",
    "print \"%s distinct communities were detected \\n\" % len(communities.keys())\n",
    "\n",
    "print \"Here are the top 10 most populous communities:\\n\"\n",
    "for i,j in sorted(community_size,key=lambda x: x[1])[::-1][:10]:\n",
    "    print \"Community %s has %s members\" % (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTAGRAM HASHTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import dateutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-945d3457ea32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#tags_df.columns = ['text', 'date']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmulti_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "def get_tags(text):\n",
    "    '''Return unique lists of hashtags from twitter text, by splitting text\n",
    "    and returing unique strings that start with #'''\n",
    "    tags = list( set(tag for tag in text.split() if tag.startswith('#')) )\n",
    "    to_remove=\"$:;,.?!/\\\\*%@#\"\n",
    "    return map(lambda x: ''.join( c for c in x if  c not in to_remove ),tags)\n",
    "\n",
    "tags = []\n",
    "verbose=False\n",
    "for i in open(\"hashtag_list01.csv\", 'r').readlines():\n",
    "    if i==\"\\n\":\n",
    "        next\n",
    "    else:\n",
    "        try:\n",
    "            data = json.JSONDecoder().raw_decode(i)\n",
    "            hash = list(data)[0]\n",
    "            text = hash[\"text\"].rstrip('\\n\\r') # remove newline characters\n",
    "            clean_text = text.encode(\"ascii\",\"ignore\") # removes unicode\n",
    "            date = dateutil.parser.parse(hash[\"created_at\"])\n",
    "            tags.append([map(lambda x:x.upper(),get_tags(clean_text)),date]) # make hashtags uppercase to avoid case-senstive duplications\n",
    "        except:\n",
    "            if verbose==True: print( \"No useable data on this line:\"), hash\n",
    "            continue\n",
    "\n",
    "tags.sort(key=lambda l_t: l_t[1]) # make sure the data are date sorted, otherwise searching will be not be fast\n",
    "\n",
    "#tags_df = pd.DataFrame(tags)\n",
    "#tags_df.columns = ['text', 'date']\n",
    "multi_index = pd.MultiIndex(levels = [ ['text', 'date']], ['a', 'b']], labels = [[0, 0, 1, 1], [0, 1, 0, 1]])    \n",
    "df = pd.DataFrame(columns=multi_index)\n",
    "\n",
    "\n",
    "tags_df = tags_df.set_index('date')\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "tag_text = flatten(tags_df[\"text\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imread' from 'scipy.misc' (/Applications/miniconda3/envs/espacios_ya/lib/python3.8/site-packages/scipy/misc/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-09b586078243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'imread' from 'scipy.misc' (/Applications/miniconda3/envs/espacios_ya/lib/python3.8/site-packages/scipy/misc/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imread' from 'scipy.misc' (/Applications/miniconda3/envs/espacios_ya/lib/python3.8/site-packages/scipy/misc/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0f5e454e6c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mred_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'imread' from 'scipy.misc' (/Applications/miniconda3/envs/espacios_ya/lib/python3.8/site-packages/scipy/misc/__init__.py)"
     ]
    }
   ],
   "source": [
    "def red_col(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return random.choice(['hsl(49, 100%, 81%)', 'hsl(34, 99%, 65%)', 'hsl(8, 87%, 53%)'])\n",
    "\n",
    "cloud_mask = imread(\"brain.png\")\n",
    "image_colors = ImageColorGenerator(cloud_mask)\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=cloud_mask)\n",
    "wordcloud = wc.generate(\" \".join(tag_text))\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(wc.recolor(color_func=red_col))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ff45e8f0cc06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal_word_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_word_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tag_text' is not defined"
     ]
    }
   ],
   "source": [
    "total_word_count=[]\n",
    "for i in set(tag_text):\n",
    "    total_word_count.append((i,tag_text.count(i)))\n",
    "    \n",
    "number = 10\n",
    "top_all = dict(sorted(total_word_count, key=lambda x: x[1])[-number:])\n",
    "\n",
    "for i in top_all.keys():\n",
    "    print (\"The %s hashtag was seen about %d times\" % (i,top_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_groups(df, sec):\n",
    "    '''Find groups of hashtags in 60 second windows.'''\n",
    "    time_now = datetime.datetime.now()\n",
    "    start = df.index.searchsorted(time_now - timedelta(seconds=sec))\n",
    "    end = df.index.searchsorted(time_now)\n",
    "    return df.ix[start:end]\n",
    "\n",
    "top_rank = {}\n",
    "for i in top_all.keys():\n",
    "    top_rank[i] = []\n",
    "\n",
    "start = tags_df.index.searchsorted(datetime.datetime.utcfromtimestamp(tags_df.ix[0:1].index.values.tolist()[0]/1e9))\n",
    "end = 0\n",
    "\n",
    "while end < len(tags_df.index):\n",
    "    end = tags_df.index.searchsorted(datetime.datetime.utcfromtimestamp(\n",
    "            tags_df.ix[start:start+1].index.values.tolist()[0]/1e9) + \n",
    "            timedelta(seconds=3600))\n",
    "    \n",
    "    new_df = tags_df.ix[start:end]\n",
    "    hash_list = flatten(new_df[\"text\"].values.tolist())\n",
    "\n",
    "    hash_list_counts =  dict([(x,hash_list.count(x)) for x in set(hash_list)])\n",
    "    raw_counts = [(x,hash_list_counts[x]) if x in hash_list_counts.keys() \n",
    "                   else (x,0) for x in top_rank.keys()]\n",
    "    \n",
    "    ranks = zip(range(1,len(raw_counts)+1),sorted(raw_counts,key=lambda x: x[1])[::-1])\n",
    "\n",
    "    for i in ranks:\n",
    "        top_rank[i[1][0]].append(i[0])\n",
    "\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.graph_objs import *#Scatter, Layout\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "def data_plot(rank_dict):\n",
    "    container = []\n",
    "\n",
    "    for i in rank_dict.keys():\n",
    "        ranks = rank_dict[i]\n",
    "        container.append(Scatter(x=range(1,len(ranks)),y=ranks,name=i))\n",
    "        \n",
    "    layout= dict(\n",
    "        xaxis = dict(title = 'Hour'),\n",
    "        yaxis = dict(title = 'Relative Rank',autorange='reversed'),\n",
    "        legend=dict(\n",
    "          y=0.5,\n",
    "          font=dict(size=12)))\n",
    "\n",
    "    return (container,layout)\n",
    "\n",
    "data,layout=data_plot(top_rank)\n",
    "fig = Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from IPython import display\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "def blu_col(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return random.choice(['hsl(107, 50%, 91%)', 'hsl(135, 44%, 76%)', 'hsl(198, 56%, 53%)','hsl(198, 56%, 53%)'])\n",
    "\n",
    "def create_graph(list_of_lists):\n",
    "    '''given a list conprising of lists of hashtags, construct a graph.\n",
    "    If the list is greater than 2 or more, make sure to create all possible\n",
    "    edges given the hashtag combination.  If the list is 0, do nothing and \n",
    "    if it is of size 1 then just create a node.'''\n",
    "    \n",
    "    G=nx.Graph()    \n",
    "    for i in list_of_lists:\n",
    "        if len(i)>1:\n",
    "            G.add_edges_from(list(itertools.combinations(i,2)))\n",
    "        elif len(i)==0:\n",
    "            continue\n",
    "        elif len(i)==1:\n",
    "            G.add_node(*i)\n",
    "    return G\n",
    "\n",
    "class StdOutListener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        out = []\n",
    "        verbose=False\n",
    "        try:\n",
    "            data = json.JSONDecoder().raw_decode(data)\n",
    "            hash = list(data)[0]\n",
    "            text = hash[\"text\"].rstrip('\\n\\r') # remove newline characters\n",
    "            clean_text = text.encode(\"ascii\",\"ignore\") # removes unicode\n",
    "            date = datetime.datetime.now()\n",
    "            out.append([map(lambda x:x.upper(),get_tags(clean_text)),date]) # make hashtags uppercase to avoid case-senstive duplications\n",
    "        except:\n",
    "            if verbose==True: print (\"No useable data on this line:\"), hash\n",
    "\n",
    "        out.sort(key=lambda l_t: l_t[1]) # make sure the data are date sorted, otherwise searching will be not be fast\n",
    "\n",
    "        if 's_t' not in globals():\n",
    "            #global s_t\n",
    "            s_t = pd.DataFrame(out)\n",
    "            s_t.columns = ['text', 'date']\n",
    "            s_t = s_t.set_index('date')\n",
    "        else:\n",
    "            #global s_t\n",
    "            s_t_2 = pd.DataFrame(out)\n",
    "            s_t_2.columns = ['text', 'date']\n",
    "            s_t_2 = s_t_2.set_index('date')\n",
    "            s_t = pd.concat([s_t,s_t_2])\n",
    "        \n",
    "        g =  create_graph(get_time_groups(s_t,120)[\"text\"].values.tolist())\n",
    "        to_prune = nx.isolates(g) # find isolate nodes\n",
    "        g.remove_nodes_from(to_prune) # remove isolate nodes\n",
    "        \n",
    "        if nx.number_of_nodes(g) == 0: # if empty graph, keep listening\n",
    "            return True\n",
    "        try: # try to compute centrailty, if no convergence, move on\n",
    "            ev_cent=nx.eigenvector_centrality(g,max_iter=10000) # compute eigenvector centrality\n",
    "        except:\n",
    "            return True\n",
    "            \n",
    "        ev_tuple = []\n",
    "        for i in ev_cent.keys():\n",
    "            ev_tuple.append((i.replace(\"#\", \"\"),ev_cent[i]))\n",
    "            \n",
    "        global time\n",
    "        \n",
    "        if (datetime.datetime.now() - time) > timedelta(seconds=1): # allow refresh every ~x seconds, if tweets come in\n",
    "            cloud_mask = imread(\"tweet.png\")\n",
    "            image_colors = ImageColorGenerator(cloud_mask)\n",
    "            wc = WordCloud(background_color=\"white\", max_words=2000, mask=cloud_mask)\n",
    "            wordcloud = wc.generate_from_frequencies(ev_tuple)\n",
    "            plt.figure(figsize=(12,12))\n",
    "            plt.imshow(wc.recolor(color_func=blu_col))\n",
    "            plt.axis(\"off\")\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            time  = datetime.datetime.now()\n",
    "            \n",
    "        return True        \n",
    "\n",
    "        def on_error(self, status):\n",
    "            print (status)\n",
    "            \n",
    "consumer_key = '[your customer key here]'\n",
    "consumer_secret = '[your consumer secret here]'\n",
    "access_token = '[your access token here]'\n",
    "access_token_secret = '[your access token secret here]'\n",
    "\n",
    "if 's_t' in globals():\n",
    "    del(s_t)\n",
    "\n",
    "time = datetime.datetime.now()\n",
    "l = StdOutListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "stream = Stream(auth, l)\n",
    "stream.filter(track=['data science', 'big data', 'machine learning'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
